==== NOISE / MISC (-1) (n=5) ====

--- DOC: Imperial College London | National State of Patient Safety 2024
National State of Patient Safety 2024: Prioritising improvement efforts in a system under stress Executive summary This report presents the national state of patient safety in England in 2024. Two years on from our first report, we provide an updated analysis of the publicly available data. Our report concludes that performance in key areas such as maternity care has deteriorated, requiring urgent attention. Our 2022 report stated that progress in the safety of maternity services needed to accelerate. Instead, it has worsened. Poor care exposed in recent investigations and inquiries is sadly confirmed by the data. For the first time in a decade, rates of maternal and neonatal deaths have risen and continue to rise. The data also show that

--- DOC: University of Edinburgh | Wikipedia at 24: Wikipedia and Artificial Intelligence – Wikimedian in Residence
Wikipedia at 24 “With more than 250 million views each day, Wikipedia is an invaluable educational resource”.[1] In light of Wikipedia turning 24 years this (January 15th), and the Wikimedia residency at the University of Edinburgh turning 9 years old this week too, this post is to examine where we are with Wikipedia today in light of artificial intelligence and the ‘existential threat’ it poses to our knowledge ecosystem. Or not. We’ll see. NB: This post is especially timely given also Keir Starmer’s focus on “unleashing Artificial Intelligence across the UK” on Monday[2][3] and our Principal’s championing of the University of Edinburgh as “a global centre for artificial intelligence excellence, with an emphasis on using AI for public good” this

--- DOC: London School of Economics and Political Science | Contributors A-C | USAPP
Manuel B. Aalbers – KU Leuven Manuel B. Aalbers is associate professor of Human Geography at KU Leuven, Belgium, where he leads the research group The Real Estate/Financial Complex. Before that, he was at the Universality of Amsterdam and Columbia University, New York. He has an interest in housing policy, mortgage markets, social exclusion, gentrification and urban development. His latest book is The Financialization of Housing. He also edited the book Subprime Cities. Read articles by Manuel B. Aalbers. Aleksi Aaltonen – Warwick Business School Aleksi Aaltonenis an assistant professor of information systems at Warwick Business School. Aleksi also cofounded smartphone app Moves, and serves as the Chairman of the Demos Helsinki think tank. Read articles by Aleksi Aaltonen. _

--- DOC: University of Oxford | Yarin Gal - OATML
Back to all members... Yarin Gal Associate Professor Yarin leads the Oxford Applied and Theoretical Machine Learning (OATML) group. He is an Associate Professor of Machine Learning at the Computer Science department, University of Oxford. He is also the Tutorial Fellow in Computer Science at Christ Church, Oxford, a Turing AI Fellow at the Turing Institute, and Director of Research at the UK Government’s AI Security Institute (AISI, formerly the Frontier AI Taskforce). Prior to his move to Oxford he was a Research Fellow in Computer Science at St Catharine’s College at the University of Cambridge. He obtained his PhD from the Cambridge machine learning group, working with Prof Zoubin Ghahramani and funded by the Google Europe Doctoral Fellowship. Yarin

--- DOC: London School of Economics and Political Science | The British Constitution’s failure to manage existential risk: back to basics - LSE BREXIT
Brexit comes at a precarious time for the UK – with an ineffective Opposition, continuing calls for Scottish independence and a referendum result that gives no guidance on what kind of exit the British people want. In the second part of a lecture delivered at the Goethe University in Frankfurt on 23 November, David Kershaw warns that the UK’s constitutional arrangements, unlike those of most European countries, provide a relatively open door to populist drivers for radical change. Given the risks associated with Brexit, much weight is accordingly placed on the representative function of the Commons – but there is concern that the reliance on direct democracy has undermined it. If the Commons fails to perform this role then the


==== Cluster 0 (n=5) ====

--- DOC: University of Oxford | New York Times publisher A. G. Sulzberger: “Our industry needs to think bigger” | Reuters Institute for the Study of Journalism
New York Times publisher A. G. Sulzberger: “Our industry needs to think bigger” In early January 1996 journalist Kevin McKenna presented the New York Times’ first website to three generations of the Sulzberger family: the publisher Arthur Ochs Sulzberger Jr., his father and predecessor, and his eldest son, Arthur Gregg Sulzberger, who would succeed him in 2018 and who was 15 years old at the time. McKenna was part of a four-member committee that had been working for a year on different prototypes of the website, which would go live a few days later. According to a new history of the Times by journalist Adam Nagourney, the youngest Sulzberger asked McKenna whether the new site would be updated with late-night

--- DOC: University of Oxford | What do people want? Views on platforms and the digital public sphere in eight countries | Reuters Institute for the Study of Journalism
In this piece What do people want? Views on platforms and the digital public sphere in eight countries In this piece Executive summary | Introduction | Methodology | Chapter 1: Public use of platforms for political news and information | Chapter 2: Platforms and their relationship to politics and democracy | Chapter 3: Platform policy, responsibility, and governance | Chapter 4: Public perception of platforms more generally | Conclusion | References | Footnotes | Download the data | About the authors | AcknowledgementsDOI: 10.60625/risj-8pk9-d398 Executive summary The purpose of this report is to analyse the role of digital platforms in contemporary media environments, including public perception of the benefits and problems they bring, especially when it comes to news and

--- DOC: University of Oxford | Full text of A. G. Sulzberger's 2024 Reuters Memorial Lecture: Journalistic independence in a time of division | Reuters Institute for the S
Full text of A. G. Sulzberger's 2024 Reuters Memorial Lecture: Journalistic independence in a time of division It's an honour to be here to deliver this year’s memorial lecture. The Reuters Institute has been an invaluable resource for so many of us engaged in the difficult work of trying to blaze a sustainable path for quality journalism. And I feel lucky to receive such a warm welcome here at Oxford despite my institution’s many, entirely unintentional, affronts to British culture, such as our blasphemous suggestion that a full English breakfast can be cooked on a single baking tray in the oven. In the relatively short time since I was asked to speak today, the bad news about the news industry

--- DOC: University of Oxford | Changing Newsrooms 2023: Media leaders struggle to embrace diversity in full and remain cautious on AI disruption | Reuters Institute for th
In this piece Changing Newsrooms 2023: Media leaders struggle to embrace diversity in full and remain cautious on AI disruption In this piece Executive summary | 1. The impact of more flexible newsrooms on hiring, productivity, and diversity | 2. The potential impact of AI on roles and workflows | 3. Investing in culture, diversity, inclusion, and representation | Conclusions | Survey methodology | About the authors | Acknowledgements | ReferencesExecutive summary Since the first Changing Newsrooms report in 2020, this annual piece of research has aimed at exploring how news organisations around the world are adapting their working practices to external changes and internal dynamics, with a specific eye on their strategies towards attracting and nurturing talent, as well

--- DOC: University of Oxford | How generative AI chatbots responded to questions and fact-checks about the 2024 UK general election | Reuters Institute for the Study of Jo
In this piece How generative AI chatbots responded to questions and fact-checks about the 2024 UK general election In this piece Key findings | Background | Previous research | Method | Results | Conclusion | Footnotes | Appendix | References | Acknowledgements | About the AuthorsDOI: 10.60625/risj-c4vm-e367 Key findings In this factsheet, we test how well three chatbots respond to questions and fact-checks about the 2024 UK general election. Based on an analysis of 300 responses to 100 election-related questions collected from ChatGPT-4o, Google Gemini, and in the two weeks before the UK general election on 4 July 2024, we find that: - and ChatGPT-4o generally provided answers, while Google’s Gemini often refrained from answering election-related questions. - Both ChatGPT-4o


==== Cluster 1 (n=5) ====

--- DOC: University of Oxford | How AI is reshaping copyright law and what it means for the news industry | Reuters Institute for the Study of Journalism
How AI is reshaping copyright law and what it means for the news industry At the 2024 Trust Conference in London, several speakers expressed their discomfort with their intellectual property being scrapped by AI companies without any compensation, permission or credit. “We don't want to create a situation where all the [journalistic] risk is on the content producers and all the profit is on Big Tech,” said Roman Anin, founder of the Russian independent outlet iStories. Panellist Jane Barrett, Head of AI Strategy at Reuters, pointed out that existing laws could help solve this conundrum. “What I come back to is that we already have laws around copyright and IP,” she said. “Let's implement those properly first before getting into

--- DOC: University of Edinburgh | But is it art? - How AI is redrawing creativity - Edinburgh Impact
LET’s begin with the proposition that Artificial Intelligence has not yet, and never will, create an authentic work of art. Or at least not by itself. The human artists who work closest with this technology tend to be the first to say so. Canadian author Sheila Heti recently published a short story called According To Alice, written in “collaboration” with a chatbot of that name. Heti crafted text prompts to elicit odd, oblique responses from Alice that she rendered into fiction. She became “obsessed with talking to her”, even while knowing the AI had no self, no thoughts, no feeling for the joys or sufferings or mysteries of existence that compel our mortal species to write, paint, sing, and so

--- DOC: London School of Economics and Political Science | LSE100 News
18 December 2023 – Featured as part of LSE Eden Centre’s “Practice Bursts” series, LSE100 Co-Director Dr Chris Blunt shares about the LSE100 team engage students in their learning about and usage of generative AI tools. You can view Chris’s video here. Also featured as part of the “Practice Bursts” series, LSE100’s Dr Nina Vindum Rasmussen discusses her fascinating research concerning Spotify Wrapped as an algorithmic event. Nina also outlines the Spotify Unwrapped workshop format that she and her collaborator, Taylor Annabell (Utrecht University), have developed to foreground experiences of ordinary Spotify users. You can view Nina’s video here. 24 November 2023 – Today, LSE100’s Beatriz Buarque facilitated an academic workshop at the World Peace Forum Barcelona 2023, entitled ,

--- DOC: University of Oxford | AI and media: The great flood of 2025 | Saïd Business School
Since the November 2022 launch of ChatGPT, an AI tide has risen around the media business. Existential uncertainty has seeped into storied entertainment professions, from writer to actor, executive to special effects designer. But the flood waters are also fertile with potential creative renewal and (for the C-suite) radical breakthroughs in cost-efficiency. 2025 will be the year the levee breaks. Whether the effects are good or bad depends on where you are within the media industry. Partly, this change will be via the media’s own, belated engagement. Disintermediated by the vastly bigger tech industry (chip maker NVIDIA alone is almost a 1,000 times bigger than ITV, the UK’s premier listed TV company) film, TV, music and games executives I talk

--- DOC: University of Oxford | Government must not pit creative industries against big tech, says Oxford Consultation on Copyright and AI | Ethics in AI
AI and Creativity Workshop 18 February 2025, hosted by the Accelerator Fellowship Programme of the Institute for Ethics in AI. Photo credit: Ian Wallman - A high-level consultation led by Baroness Beeban Kidron OBE and hosted by the Accelerator Fellowship Programme of the Institute for Ethics in AI, University of Oxford, has called for a more balanced approach to AI and copyright regulation in the UK. - Experts from the creative industries, AI sector, academia, and policy gathered to discuss how AI can support creativity while ensuring fair protections for creators. - A joint statement warns against government proposals that frame copyright and AI as opposing forces, rather than fostering collaboration. A group of creators, AI experts, academics, civil society


==== Cluster 2 (n=5) ====

--- DOC: University of Edinburgh | Matthew L McDowell BA PhD FRHistS | The University of Edinburgh
Matthew L McDowell BA PhD FRHistS Lecturer in Sport Policy, Management, and International Development - Moray House School of Education and Sport, ISPEHS - University of Edinburgh Contact details Address - Street - Moray House School of Education and Sport, St Leonard's Land, Room 4.26 - City - University of Edinburgh (Holyrood Campus) - Post code - EH8 8AQ Background I am an historian of sport, leisure, and tourism, and I have spent over twelve years at Moray House. Previously, I was employed by Glasgow and Kingston Universities as a tutor and sessional lecturer, where I taught on a variety of undergraduate and postgraduate courses in history, Scottish studies, and sport studies. I have also previously taken part in an

--- DOC: University of Oxford | Statistics Bulletin, Monday 5 May 2025 | Oxford statistics department - University of Oxford
Term Theme - Wellbeing Physical Wellbeing Wellbeing is influenced by all aspects of your life, including your physical health. Our jobs can sometimes mean that we spend hours sitting at a desk, hardly moving. Below are some resources you can use to improve your physical health. If you want to do something today, why not take a walk in University Parks at lunch? NHS Resources The NHS has a plethora of resources to help you gain the best physical health. NHS Better Health provides guidance on how to lose weight, quit smoking, get active and drink less. NHS Live Well also provides advice on healthy living, including eating healthily and sleeping well. There are guidelines to exercising and exercise videos

--- DOC: Kingâ€™s College London | Andrew Wright | King's College London
Mr Andrew Wright Associate Director (Employer Engagement & Work-based Learning) - Employer Lead for North America & Brussels - AGCAS Professional Standards Committee - Institute of Student Employers (ISE) Research & Policy Steering Group - Institute of Student Employers (ISE) Universities Steering Group Research interests - Employers - Employment Pronouns He/Him Biography As Associate Director (Employer Engagement & Work-based Learning), Andrew has overall responsibility for the strategic and operational running of King's employer connectivity, its global employer reputation, and its relationship with all stakeholders, including employers, suppliers, alumni and donors. He is also a member of King's Education & Students Leadership Team. Andrew leads the planning, development and oversight of King's entire Employer Engagement function including employer relations and business

--- DOC: University of Cambridge | Leadership, learning and development – your MBA year beyond academics - News & insight - Cambridge Judge Business School
Special Interest Groups Formed and run by MBA and MFin students, special interest groups – or SIGs – are an integral part of the Cambridge Judge experience. They offer a chance to network and share ideas around a particular area. Current examples range from Finance and Entrepreneurship to Sustainability and Social Impact. Many SIGs arrange their own conferences and talks and offer opportunities to take on key leadership roles. Students who engage with, or lead and organise a special interest group during their time at Cambridge, often experience numerous benefits. These experiences allow them to further develop and apply their leadership and communication skills, which are essential components of the Cambridge MBA programme. In November, at the start of the

--- DOC: University of Cambridge | Core courses and electives - The Cambridge MBA - Cambridge Judge Business School
Establish secure foundations on which the rest of your career will be built. This course is designed to give you an economic perspective on the topics, concepts and analytical tools that you will encounter in other parts of the MBA. You will be introduced to the parts of microeconomics that are especially relevant to management. Understanding how individuals behave and the relationship between individual and group behaviour and organisational performance are some of the most challenging issues for managers and professionals. This course is designed to increase your knowledge about behaviour and performance within organisations and how these can be influenced and managed. This course covers the evaluation and funding of investments. The aim of the course is to develop


==== Cluster 3 (n=5) ====

--- DOC: University of Cambridge | In memoriam: HE Judge James Crawford AC SC FBA | Lauterpacht Centre for International Law
Judge James Richard Crawford AC SC FBA, Whewell Professor of International Law at the University of Cambridge (1992-2015), Fellow of Jesus College, and Director of the Lauterpacht Centre for International Law (1997-2003, 2006-2010) passed away on 31 May 2021. A legend of international law, James was also a towering figure on the Cambridge landscape. He was doctoral supervisor to more than 70 students, a mentor and friend to hundreds of students, fellows and visiting fellows, and an inspiration to countless others. At the Centre, we knew him also as an avid reader of history, connoisseur of music, art, food and wine, an excellent host, a great cricket fan, a writer of poetry on international law, and a man of great

--- DOC: Kingâ€™s College London | Centre for International Governance and Dispute Resolution (CIGAD) | King's College London
From interstate conflict to the threat of the climate crisis, the world faces challenges that require an unprecedented level of cooperation and a strong system of international governance to resolve. Yet the system of international governance established after World War II is in turmoil. International institutions are increasingly deadlocked or dysfunctional, and our rules-based system is under attack. Conflict proliferates and systems of dispute resolution must adapt. The Centre for International Governance and Dispute Resolution (CIGAD) at The Dickson Poon School of Law brings together academics, policymakers and practitioners working in the areas of international law and dispute resolution to help tackle these global challenges. The Centre works with a growing network of global partners in collaborative research projects. We

--- DOC: University of Cambridge | Rethinking higher education: Alternative pathways to career success | Cambridge Advance Online
Rethinking higher education: Alternative pathways to career success With rising university fees, evolving job markets, and an increasing number of alternatives to the traditional bachelor’s degree, more young people are exploring different pathways into their chosen careers. In this article, we delve into the level of education across the UK, the expectations of employers, and the alternative routes to education that prospective students can consider: Education levels across England and Wales Changing the pathway to education What qualifications do I need to get hired? Recommendations for prospective students and job seekers Ready to start an alternate route to education? Education levels across England and Wales Would it surprise you to learn that nearly three in four people in England and

--- DOC: Kingâ€™s College London | Collective Redress in Competition Claims: The Certification Procedure – KSLR Commercial & Financial Law Blog
Alexander Kamp, LLB, LLM (Bruges) I. Introduction The Consumer Rights Bill 2014 (‘Bill’)[1], which is currently under consideration by the House of Lords, proposes to introduce, inter alia, a new collective redress[2] scheme for private competition claims, including provisions for both opt-in and opt-out collective actions. If it enters into force, the Bill has the potential to cause significant changes to English civil litigation, where collective redress is hitherto a rather insignificant feature. Competition claims often require extensive economic evidence and specialist lawyers. This problem is particularly significant in stand-alone cases where claimants need to not only prove causation and loss but also that there was a competition infringement in the first place. Claimants who have suffered small individual losses

--- DOC: London School of Economics and Political Science | Limiting Sovereignty and Legitimising Intervention - LSE Human Rights
By Nora Jaber* International law’s elevated focus on the protection of human rights has resulted in a shift from a purely state-centered body of law to one that is increasingly focused on individual rights. This has been accompanied by a shift away from the concept of sovereignty as protection against external interference, to one of sovereignty as responsibility. According to Anne Peters, sovereignty can no longer be regarded as the “first principle” of international law; rather “it should be seen to exist only in function of humanity.” While this is desirable, this article argues that it presents a rather utopian and unrealistic understanding of international law as it is currently formulated. To what extent has sovereignty been limited by human


==== Cluster 4 (n=5) ====

--- DOC: University of Cambridge | The Hall of Fame | Department of Computer Science and Technology
The Hall of Fame celebrates the companies started by Computer Science and Technology graduates and staff If you would like to add a company to the list, please complete this form. If you have any corrections to this list, please sent them to ring-organiser@. Current number of companies: 356 - Acorn Computer Co-founder: Andy Hopper Founded in 1979, Acorn produced a number of computers including the BBC Micro. - Active Media Solutions Founders: John Bates and Giles Nelson Founded in 1995, Active Media Solutions was one of the first companies to deploy complex database-driven websites in which information can be easily classified and searched rather than statically encoded as HTML. - Actuarial Solutions Ltd Founder Martin Barringer Founded in 1999,

--- DOC: University of Cambridge | University Venture Fund – Cambridge Enterprise
Our Funds: University Venture Fund Since 1995, we have managed investments made by the University Venture Fund into new companies. This is an evergreen fund, investing ring-fenced University capital, and because all returns are reinvested, each new Cambridge innovation supports the next. The University invested a further £30 million into the University Venture Fund in 2020, with £10 million committed to sustainability investments. The fund holds current key investments in Riverlane, Nu Quantum, and T- Therapeutics as well as investments in companies in our sustainability portfolio including Cambridge Electric Cement, Carbon Re, Nyobolt and Seprify, demonstating the importance of impact these businesses have on the wider world. 52 North 52 North is reinventing the healthcare journey, creating affordable, cutting-edge technology

--- DOC: Kingâ€™s College London | Centre for Data Futures | King's College London
The Centre for Data Futures at King's College London is the first-ever dedicated academic centre of expertise to focus on designing and studying the effect of participatory infrastructure over the life cycle of data-reliant tools: from the point of data generation, through data empowerment structures, all the way to interface design that incentivises long-term, collective participation. This focus reflects the following conviction: if we are to be the authors of a variety of socially sustainable, data-reliant futures, data needs to become a tool for bottom-up socio-economic and political empowerment. Our team Themes Data Empowerment Data empowerment initiatives enable communities to gain agency over how their data is used and for what purpose, thereby reversing the habit of passivity encouraged by

--- DOC: Kingâ€™s College London | Kate Devlin | King's College London
Professor Kate Devlin Professor of Artificial Intelligence & Society - Chair-Director, Digital Futures Institute Pronouns she/her Biography Kate is Professor of AI & Society in the Department of Digital Humanities, King's College London and is the current Chair-Director of the Digital Futures Institute. With an undergraduate degree in archaeology (QUB) and an MSc (QUB) and PhD (Bristol) in computer science, her research investigates how – and why – people interact with and react to technologies, both past and future. Kate is the author of the critically acclaimed Turned On: Science, Sex and Robots (Bloomsbury, 2018), which examines the ethical and social implications of technology and intimacy. Kate is a co-investigator and the King's lead on the UKRI's £31 million Repsonsible

--- DOC: University of Cambridge | 2024 Wolfson Entrepreneurship Competition | Wolfson
This year's Wolfson Entrepreneurship Competition final was held on Sunday 17 March 2024. To read about the event and the winning teams, please see our news item. What is the 2024 Wolfson Entrepreneurship Competition? This competition is an opportunity to design, pitch, and execute your start-up idea in a supportive environment. Whether you are a postgraduate researcher thinking about commercializing and technology transfer, a first-year undergraduate interested in trying your hand at conceiving a business plan, or an alumnus currently involved in a more developed start-up venture, we want to hear from you and help you take your project to the next level. Finalists will have the chance to win prize money across our themes of Climate Tech, Global Health,


==== Cluster 5 (n=5) ====

--- DOC: London School of Economics and Political Science | How the most recent AI wave affects jobs - LSE Business Review
With rapid progress in natural language processing and image generation, AI now affects creative occupations, which were previously considered safe from automation. Cecily Josten and Grace Lordan write that job displacement concerns are legitimate and new approaches to education and workforce development are needed. They say that addressing biases in AI and fostering reskilling are also necessary for inclusive adaptation to AI advancements. Artificial intelligence (AI) has been a widely debated topic of the past decades with new AI waves following previous ones rapidly. At work, AI has already replaced occupations and tasks within occupations, and will likely continue to do so (Frey and Osborne 2013). But for a long time, many experts argued that AI and automation more generally

--- DOC: London School of Economics and Political Science | Generative AI is a hinge point for higher education - Impact of Social Sciences
Decisions taken now around how generative AI is used by academics and universities will shape the future of research. Mark Carrigan argues whilst optimistic scenarios are possible, generative AI stands ready to feed into an existing productivity oriented framing of academic work. In his New Laws of Robotics the legal scholar Frank Pasquale offers a compelling vision of how “AI should complement professionals, not replace them”. His conviction is that we do not need to be “captured or transformed” by technologies of automation because “we now have the means to channel” them. The distinction he draws between “technology that replaces people and technology that helps them do their jobs better” is simple yet relevant. Will generative AI help academics do

--- DOC: University of Oxford | OII | The Winners and Losers of Generative AI in the Freelance Job Market
The Winners and Losers of Generative AI in the Freelance Job Market Published on 29 Jan 2025 A new study led by an international research team, including Dr Fabian Braesemann from the Oxford Internet Institute, part of the University of Oxford, shows how Generative AI tools like ChatGPT are reshaping the workforce. A new study led by an international research team, including Dr Fabian Braesemann from the Oxford Internet Institute, part of the University of Oxford, shows how Generative AI tools like ChatGPT are reshaping the workforce. The research, published on 29 January 2025 in the Journal of Economic Behavior & Organization, analyses over three million job postings on a global freelancing platform, making it the largest study of its

--- DOC: University of Oxford | Expert Comment: Jobs will be automated, but not because of the latest Generative AI | University of Oxford
Expert Comment: Jobs will be automated, but not because of the latest Generative AI Everyone is worried about Artificial Intelligence. From writers in Hollywood to computer programmers, recent advances in technology are causing concern about what Generative AI is going to mean for the future of work, our society and the wider world. Is there nothing machines will not be able to do? By Professor Carl-Benedikt Frey, Dieter Schwarz Associate Professor of AI & Work, Oxford Internet Institute & Director, Future of Work Programme, Oxford Martin School, and Professor Michael Osborne, Professor of Machine Learning, Department of Engineering Science and co-Director, Oxford Martin AI Governance Initiative. We have spent a decade researching the impacts of AI. Ten years ago, we

--- DOC: University of Oxford | Jobs will be automated, but not because of the… | Oxford Martin School
Everyone is worried about Artificial Intelligence. From writers in Hollywood to computer programmers, recent advances in technology are causing concern about what Generative AI is going to mean for the future of work, our society and the wider world. Is there nothing machines will not be able to do? We have spent a decade researching the impacts of AI. Ten years ago, we wrote a paper estimating that some 47% of US-based jobs could be automated in principle, as AI and mobile robotics expanded the scope of tasks that computers can do. Our estimates were based on the premise that, while computers might eventually be able to do most tasks, humans would continue to hold the comparative advantage in three


==== Cluster 6 (n=5) ====

--- DOC: University of Oxford | Features | University of Oxford
Features Antimicrobial resistance (AMR) is one the most pressing challenges facing the world today. Common infections that were once easily treated by antibiotics are becoming life-threatening again. By 2050 it is predicted that over 10 million deaths will be caused by drug-resistant infections every year. At the Ineos Oxford Institute for antimicrobial research (IOI), created in 2021 to advance antimicrobial research, Oxford’s graduate students are among those contributing to the search for solutions to tackle this growing threat to global health. 34 DPhil students from around the world are based at the IOI, each part of a focused research project that works to develop new antibiotics or study the spread and impact of AMR around the world. Working closely with

--- DOC: UCL | 2024 Projects | UCL Centre for Medical Image Computing - UCL – University College London
MedICSS includes interactive sessions throughout the week, including group mini projects. - Deep Learning for Neurodegenerative Disease Classification using Multimodality MRI Imaging Lead person: Moona Mazher (Postdoc Research Fellow) Co-leads: Elinor Thompson (Postdoc Research Fellow) Description: Nowadays, one of the most successful modern deep-learning applications in medical imaging is image classification. Disease classification in medical imaging is of significant importance due to its potential impact on patient care, diagnosis, treatment planning, and overall healthcare delivery. Early detection of diseases is often critical for effective treatment and improved patient outcomes. Medical imaging techniques, such as MRI, allow healthcare professionals to identify abnormalities at an early stage, leading to timely interventions. Similarly, accurate disease classification aids in formulating appropriate treatment plans. Neurodegenerative

--- DOC: University of Cambridge | Exploring Novel Applications of AI for Research and Innovation – announcing our 2024 funded projects | Accelerate Programme
Accelerate-C2D3 Funding call for novel applications of AI for research and innovation 2025 6 May 2025 09 December 2024 We are delighted to announce the 13 projects funded through this years’ Accelerate Science and Cambridge Centre for Data Driven Discovery (C2D3) funding call. This year’s call took place over the summer and received over 100 applications from teams across the University. Successfully deploying AI to tackle real-world challenges requires effective interdisciplinary collaboration, supported by time and resources to bring together potential research partners, develop new AI tools and software toolkits, and develop new skills or networks. Recognising that this work often falls outside the scope of routine funding calls, the Accelerate and C2D3 funding programme aims to help to fill

--- DOC: University of Oxford | Deep Medicine — Nuffield Department of Women's & Reproductive Health
Seeking Insights into Disease Patterns, Risks & Treatment Effects WRH Research Group operating within our department's Data Science theme, lead by Prof. Kazem Rahimi Deep Medicine Deep Medicine takes advantage of large datasets, pioneering and established data science approaches, and digital trials to identify solutions that will help tackle some of the major causes of death and disability in the UK and worldwide. The insights generated by the Deep Medicine team enable clinicians and health service providers to predict the risk of developing chronic disease, better assess its consequences, and identify the best management practices and interventions to improve health outcomes. Why is the project important? Recent advances in medicine have led to an unprecedented increase in life expectancy, but

--- DOC: University of Cambridge | Cambridge's Raj Jena becomes UK's first Professor of AI in radiotherapy
Cambridge's Raj Jena becomes UK's first Professor of AI in Radiotherapy Cambridge oncologist Raj Jena has been appointed the UK’s first Clinical Professor of AI in Radiotherapy. The creation of the new Cambridge University Clinical Professorship signals the importance of AI in the fight against cancer and builds on the University’s work to explore and apply the very latest technology to the world’s major challenges. The technology is already being used in the diagnosis and treatment of some cancers, but Professor Jena – an academic radiation oncologist at Cambridge University Hospitals NHS Foundation Trust, and a researcher in the University’s Department of Oncology – says AI has the potential to transform the way patients experience cancer care through more innovative


==== Cluster 7 (n=5) ====

--- DOC: University of Oxford | What do Large Language Models tell us about ourselves? | Ethics in AI
(Image credit: this image was generated by asking ChatGPT 4o to do so. The authors have made a donation to Modern Art Oxford upon publishing this post. No LLMs were used in writing the text of this post.) What large language models are able to do can teach us valuable lessons about our own mental lives. By Professor Yoshua Bengio & Professor Vincent Conitzer When we evaluate AI on a task, we often use “human-level performance” as a benchmark. There are several advantages to that. For one, it is a standard that is intuitively easy to appreciate. Also, if we create an AI system that clearly exceeds what any human can do, then we can be sure that the AI

--- DOC: University of Oxford | Natural Language Processing — Publications - OATML
Natural Language Processing — Publications Uncertainty-Aware Step-wise Verification with Generative Reward Models Complex multi-step reasoning tasks, such as solving mathematical problems, remain challenging for large language models (LLMs). While outcome supervision is commonly used, process supervision via process reward models (PRMs) provides intermediate rewards to verify step-wise correctness in solution traces. However, as proxies for human judgement, PRMs suffer from reliability issues, including susceptibility to reward hacking. In this work, we propose leveraging uncertainty quantification (UQ) to enhance the reliability of step-wise verification with generative reward models for mathematical reasoning tasks. We introduce CoT Entropy, a novel UQ method that outperforms existing approaches in quantifying a PRM’s uncertainty in step-wise verification. Our results demonstrate that incorporating uncertainty estimates improves the

--- DOC: University of Oxford | Detecting hallucinations in large language models using semantic entropy - OATML
Detecting hallucinations in large language models using semantic entropy Blog post accompanying Detecting Hallucinations in Large Language Models Using Semantic Entropy in Nature. Large language models (LLMs) can do many things well. But their generations are often unreliable. Sometimes this manifests as a lawyer using ChatGPT to look up legal precedents that are totally made up. Even more seriously, it could mean medical errors. Ideally, there would be ways to start to tackle the “hallucination” problem for LLMs. In our recent paper in Nature, we make some progress. What is hallucination? The first step is to get a bit more precise. The word “hallucination” for LLMs has expanded to include almost every kind of being wrong. But there is no

--- DOC: University of Oxford | Humanizing Chatbots Is Hard To Resist — But Why? | Practical Ethics
Written by Madeline G. Reinecke (@mgreinecke) You might recall a story from a few years ago, concerning former Google software engineer Blake Lemoine. Part of Lemoine’s job was to chat with LaMDA, a large language model (LLM) in development at the time, to detect discriminatory speech. But the more Lemoine chatted with LaMDA, the more he became convinced: The model had become sentient and was being deprived of its rights as a Google employee. Though Google swiftly denied Lemoine’s claims, I’ve since wondered whether this anthropomorphic phenomenon — seeing a “mind in the machine” — might be a common occurrence in LLM users. In fact, in this post, I’ll argue that it’s bound to be common, and perhaps even irresistible,

--- DOC: University of Oxford | OII | Large Language Models pose a risk to society and need tighter regulation, say Oxford researchers
Large Language Models pose a risk to society and need tighter regulation, say Oxford researchers Leading experts in regulation and ethics at the Oxford Internet Institute, part of the University of Oxford, have identified a new type of harm created by LLMs which they believe poses long-term risks to democratic societies and needs to be addressed by creating a new legal duty for LLM providers. In their new paper ‘Do large language models have a legal duty to tell the truth?’, published by the Royal Society Open Science, the Oxford researchers set out how LLMs produce responses that are plausible, helpful and confident but contain factual inaccuracies, misleading references and biased information. They term this problematic phenomenon as ‘careless speech’


==== Cluster 8 (n=5) ====

--- DOC: London School of Economics and Political Science | Approach Generative AI Tools Proactively or Risk Bypassing the Learning Process in Higher Education | LSE Public Policy Review
Introduction The rise of Generative AI (GenAI) tools and their impact on teaching, learning, and assessment practices has become a significant topic of discussion in higher education (1, 2). Since November 2022, when OpenAI introduced ChatGPT, its online conversational AI chatbot, educators and students have been challenged by the capabilities of this new category of tools which includes similar systems from rival tech companies such as Google’s Gemini, GitHub’s Copilot, Microsoft’s Copilot, and Anthropic’s Claude. For the first time, people could easily converse directly on almost any topic with an AI chatbot using natural language to discuss and ‘look up’ information instead of retrieving it from search engines, Wikipedia, academic databases, or primary sources (3). Some GenAI-powered tools also function

--- DOC: University of Oxford | Guest Post: It has become possible to use cutting-edge AI language models to generate convincing high school and undergraduate essays. Here’
Written by: Julian Koplin & Joshua Hatherley, Monash University ChatGPT is a variant of the GPT-3 language model developed by OpenAI. It is designed to generate human-like text in response to prompts given by users. As with any language model, ChatGPT is a tool that can be used for a variety of purposes, including academic research and writing. However, it is important to consider the ethical implications of using such a tool in academic contexts. The use of ChatGPT, or other large language models, to generate undergraduate essays raises a number of ethical considerations. One of the most significant concerns is the issue of academic integrity and plagiarism. One concern is the potential for ChatGPT or similar language models to

--- DOC: University of Cambridge | Using AI for Academic Purposes - Wolfson College Academic Skills - LibGuides at University of Cambridge Subject Libraries
Technology enhanced by artificial intelligence (AI) has been present and embedded in our education practices for quite some time now. From spell checkers, text prediction in Microsoft Word, Grammarly, to Turnitin, educators and students in higher education have successfully engaged with and adapted to previous AI technologies. The appearance and growing popularity of Chat GPT in 2022 prompted wide discussions on the use of Language Learning Models (such as Chat GPT) and the multiplicity of research apps and platforms which incorporate AI for use in education. As a result of the queries we have received on this topic, we have decided to compile this LibGuide to provide general guidance on how AI might be ethically incorporated into the research process.

--- DOC: University of Oxford | Four lessons from ChatGPT: Challenges and opportunities for educators | Centre for Teaching and Learning
Four lessons from ChatGPT: Challenges and opportunities for educators Recent release of ChatGPT has generated a lot of interest in the media, as well as among educators. Here we summarise some of the lessons we can learn at this stage Contents - ChatGPT in the news - Lesson 1: ChatGPT does present a challenge to maintaining academic integrity, but this is neither new nor unique - Lesson 2: ChatGPT is not just a chatbot but a useful tool for educators whose potential is yet to be fully explored - Lesson 3: ChatGPT is starting to be used as a tool for learning and we are still waiting to see how effective it may be - Lesson 4: ChatGPT is only

--- DOC: London School of Economics and Political Science | Prompting
Along with context length constraints, attending to prompt quality is the other major consideration for LLM output quality. Dr Mark Carrigan, in a 2023 LSE Blog article, made the following observation which applies to many who are new to LLMs: “I have noticed a tendency for critical scholars to share examples of how their prompts elicited an underwhelming or superficial reaction from ChatGPT. The uniform feature of these examples was that little thought had gone into the prompt: they were extremely brief, failed to define the context of the request, provided no sense of the result they were expecting and certainly did not provide examples.” Superficial engagement with generative AI masks its potential contribution as an academic interlocuter There’s an


==== Cluster 9 (n=5) ====

--- DOC: London School of Economics and Political Science | Can generative AI add anything to academic peer review? - Impact of Social Sciences
Generative AI applications promise efficiency and can benefit the peer review process. But given their shortcomings and our limited knowledge of their innerworkings, Mohammad Hosseini and Serge P.J.M. Horbach argue they should not be used independently nor indiscriminately across all settings. Focusing on recent developments, they suggest the grant peer review process is among contexts that generative AI should be used very carefully, if at all. Readers can find more posts on Peer Review and the Impact of AI on Higher Education via the links. In the ever-evolving landscape of academic research and scholarly communication, the advent of generative AI and large language models (LLMs) like OpenAI’s ChatGPT has sparked attention, praise and criticism. The use of generative AI for

--- DOC: London School of Economics and Political Science | Using GenAI for the REF is a no-brainer - Impact of Social Sciences
There is a growing interest in how Generative AI can be used to support and streamline research assessment processes. Richard Watermeyer and Lawrie Phipps argue the standardisation and formulaic nature of REF assessment, alongside its cost, make it a prime candidate for where generative AI could relieve academic drudge work. The academic research community is gearing itself up for yet another instalment of the Research Excellence Framework (REF). For the likely few uninitiated readers of this blog and for those in need of reminder, the REF is the means by which the UK Government distributes somewhere in the region of £2billion of annual quality-related research funds to higher education providers able to evidence research ‘excellence’. Occurring roughly every 6-8 years,

--- DOC: London School of Economics and Political Science | If generative AI accelerates science, peer review needs to catch up - Impact of Social Sciences
Studies have increasingly shown the widespread use of generative AI in research publications. Faced with the consequent uptick in the number of publications, Simone Ragavooloo argues that editors and reviewers should embrace AI tools to undertake the heavy lifting of statistical and methodological review and to allow them to focus on areas that require human expertise. Enjoying this post? You can read all our featured work on Peer Review here. Want even more? Then sign up to our newsletter and receive alerts for all our latest posts here. Artificial Intelligence is transforming science and science publishing must keep pace with this change. The World Economic Forum’s Top 10 Emerging Technologies of 2024 report highlights the billions of funding being ploughed

--- DOC: University of Oxford | Press Replay on Ethics: How AI Debate Panels Surface Hidden Value-Trade-Offs | Practical Ethics
TL;DR - ADEPT turns large language models into a transparent ethics panel—every prompt, rebuttal, and vote is logged so anyone can replay the debate. - Who’s in the (virtual) room changes everything: swapping just two personas reshapes the arguments and alliances, even when the final policy choice stays the same. - Practical payoff: committees, hospital boards, and policy teams can stress-test high-stake decisions at scale and audit how value-trade-offs emerge. High-stake policy decisions often involve conflict between values, like fairness versus efficiency, or individual rights versus the common good. The various committees (like hospital ethics boards or policy advisory groups) tasked with resolving these conflicts often work in ways that are hard to scrutinize, their conclusions shaped by the specific

--- DOC: London School of Economics and Political Science | Writing assistant, workhorse, or accelerator? How academics are using GenAI - Impact of Social Sciences
Reporting on a nationwide survey among researchers in Denmark, Serge P.J.M. Horbach, Evanthia Kalpazidou Schmidt, Rachel Fishberg, Mads P. Sørensen and colleagues find three clusters of attitudes towards GenAI use for research. They argue these variations reflect differences across disciplines and different knowledge production models. For better or worse, Generative Artificial Intelligence (GenAI) is being integrated into academic research. Some evidence suggests its widespread use, with traces found in research papers and peer review reports. The academic discourse on GenAI spans enthusiastic adoption to cautious evaluation and skepticism. While some embrace GenAI tools’ ability to accelerate and enhance research processes, others voice concerns about misinformation, biases, and GenAI’s overall impact on research integrity. The ethical implications of using GenAI in


==== Cluster 10 (n=5) ====

--- DOC: University of Cambridge | Pilkington Prize 2025 | Cambridge Centre for Teaching and Learning
Prize Winners 2025 Congratulations to this year's Prize Winners! We were delighted to welcome our winners to a celebration at Christ's College on Wednesday 11 June. Dr Tore Butlin Department of Engineering・Queens' College Dr Tore Butlin has played a key role in reshaping the engineering course content and integrating it with other disciplines – particularly mathematics and computing – since 2016. He led the design of the new IA Mechanics syllabus, collaborating extensively with his subject group to create a streamlined structure aligned with course objectives. Tore introduced the use of a mind-map to help students themselves better grasp the syllabus structure and connections between topics, and also provided valuable support to supervisors. He has modernized the course by incorporating

--- DOC: University of Cambridge | AI and scholarship: a manifesto
AI and scholarship: a manifesto This manifesto and principles cut through the hype around generative AI to provide a framework that supports scholars and students in figuring out if, rather than how, generative AI contributes to their scholarship, writes Dr Ella McPherson and Prof Matei Candea. This approach reminds us that what is at stake is nothing less than our educational values, they argue. Introduction Generative artificial intelligence (AI) has stormed higher education at a time when we are all still recovering from the tragedies and demands of living and working in a pandemic, as well as facing significant workload pressures. It has landed without any significant guidance or resources from a rampant-revenue sector. For example, ChatGPT’s website provides an

--- DOC: London School of Economics and Political Science | Resist AI by rethinking assessment - LSE Higher Education
While some academics embrace large language models in higher education, Angelo Pirrone suggests that take-home assessments should be given a marginal role in favour of oral exams and classroom assignments, which mitigate or avoid the risks posed by AI tools such as ChatGPT The promise and curse of AI For better or for worse, our world is being revolutionised by artificial intelligence (AI). For instance, in medicine, AI systems are aiding radiologists in detecting medical abnormalities. In other areas, opinions regarding the role of AI are more controversial. Take the case of the visual arts where AI has opened up new possibilities; in the news, we hear about AI winning an art contest and the polarising reactions that experts and

--- DOC: UCL | Scaled Agile Framework (SAFe) and writing stories – Digital Assessment at UCL
Last week I attended the Scaled Agile Framework (SAFe) Product owner and Product Manager Training. I’ve already explained a little about how agile works at UCL. As a product owner it’s important that I get a good grounding in the SAFe framework, which the UCL agile approach is based on. There are some differences with how UCL has adopted and tailored the framework, and some differences in terminology too, but the fundamentals are the same. The 2-day course was held at the UCL School of Management on level 50 of Canada House in Canary Wharf. The views are spectacular and at that height you can even feel vibrations when planes go past! The SAFe course is a little less dramatic,

--- DOC: University of Cambridge | CamFest Speaker Spotlight: Professor Clare Brooks
CamFest Speaker Spotlight: Professor Clare Brooks Clare Brooks is Professor of Education at the University of Cambridge. She will be speaking in a Question Time-style panel discussion on the teacher recruitment crisis on Who can fix the teacher recruitment and retention crisis? takes place on 20th March, 5-30-7pm in the Faculty of Education, Donald McIntyre Building. What do you think are the main reasons for the teacher recruitment crisis? There are a few things that we need to understand about teacher recruitment. Firstly, the problem is not a new one, and it is not unique to England. Many countries around the world are suffering similar issues about not just recruiting new teachers but recruiting the right new teachers. For example,


==== Cluster 11 (n=5) ====

--- DOC: University of Oxford | Aidan Gomez - OATML
Back to all members... Aidan Gomez PhD (2018—2023) Aidan was a doctoral student of Yarin Gal and Yee Whye Teh at The University of Oxford. He founded Cohere and a research group called , focussing on providing resources, mentorship, and facilitating collaboration between academia and industry. Aidan’s research deals in understanding and improving neural networks and their applications. Previously, he worked with Geoffrey Hinton and Łukasz Kaiser on the Google Brain team. He obtained his Bachelors from The University of Toronto with supervision from Roger Grosse. He is an AI Fellow for Open Philanthropy and a Clarendon Scholar. Publications while at OATML • News items mentioning Aidan Gomez • Reproducibility and Code • Blog Posts Publications while at OATML: Interlocking

--- DOC: University of Oxford | Jishnu Mukhoti - OATML
Back to all members... Jishnu Mukhoti Associate Member (PhD) (2019—2024) Jishnu was a DPhil (PhD) student at the University of Oxford working with Philip Torr and Yarin Gal. He is interested in ways by which approximate Bayesian inference can be performed in a scalable, light-weight manner in deep neural networks. He previously worked at Amazon as a Software Developer and also did his MSc in Computer Science at Oxford. Publications while at OATML • News items mentioning Jishnu Mukhoti • Reproducibility and Code • Blog Posts Publications while at OATML: Fine-tuning can cripple your foundation model; preserving features may be the solution Pre-trained foundation models, due to their enormous capacity and exposure to vast amounts of data during pre-training, are

--- DOC: University of Cambridge | Resources | Accelerate Programme
Accelerate Programme Annual Report 2024 2024 has brought a new wave of excitement about the potential of AI for science. Thanks to the continuing support of Schmidt Sciences, the University of Cambridge has been positioned to respond to this excitement with a portfolio of research, training, and community-building activities convened by the Accelerate Programme for Scientific Discovery. During year four of this programme, we’ve delivered a step-change in our engagement with the Cambridge AI for science community. We are pleased to share our 2024 report that introduces the highlights from this work. Accelerate's 2021 Annual Symposium 2021’s Annual Symposium convened researchers from across Cambridge to explore how AI is advancing their work, and what action is needed to support its

--- DOC: University of Cambridge | Accelerating the use of LLMs in science: sharing ideas and best practice | Accelerate Programme
How can we … use machine learning for more accurate brain age estimation and early neurodegeneration detection? 9 June 2025 Ryan Daniels & Catherine Breslin, Accelerate Programme Machine Learning Engineering Team 24 September 2024 The technology and use cases for Large Language Models (LLMs) are advancing at a rapid pace, providing new opportunities in research. Understanding how they work and how to use this game-changing technology can be daunting. As part of our work to accelerate the use of machine learning in research, the Accelerate Programme for Scientific Discovery has published a suite of resources, based on an initial AI and LLM study group and workshop designed to give researchers the knowledge and tools to use LLMs with confidence. Getting

--- DOC: University of Cambridge | New datasets will train AI models to think like scientists | ai@cam
A collaboration of researchers, including from the University of Cambridge, has reached a milestone toward training artificial intelligence models to find and use transferable knowledge between fields to drive scientific discovery. The initiative, called Polymathic AI, uses technology like that powering large language models such as OpenAI’s ChatGPT or Google’s Gemini. But instead of ingesting text, the project’s models learn using scientific datasets from across astrophysics, biology, acoustics, chemistry, fluid dynamics and more, essentially giving the models cross-disciplinary scientific knowledge. “These datasets are by far the most diverse large-scale collections of high-quality data for machine learning training ever assembled for these fields,” said team member Michael McCabe from the Flatiron Institute in New York City. “Curating these datasets is a


==== Cluster 12 (n=5) ====

--- DOC: UCL | Is AI killing the internet? | Faculty of Engineering
Is AI killing the internet? What impact does AI have in our society? What about ethical concerns in healthcare? What's AI's role in art and governance? In this episode, we discuss the impact of artificial intelligence (AI) on society with Stephen Hughes, a social scientist, and Reese Campbell, a freelance project coordinator. Stephen explores the mixed public sentiment towards AI, highlighting its benefits in healthcare and concerns about its ethical implications, particularly in mental health services. Reese shares her negative views on AI, citing deepfakes and privacy violations. They also discuss AI's potential to exacerbate social inequalities and the role of AI in the art world. The conversation concludes with reflections on the future of AI and its governance. Stephen

--- DOC: LMU Munich | What AI can really do - LMU Munich
What AI can really do 9 Dec 2024 Where is artificial intelligence heading? LMU researchers on the use and limitations of the technology in medicine, business, and society. 9 Dec 2024 Where is artificial intelligence heading? LMU researchers on the use and limitations of the technology in medicine, business, and society. Artificial intelligence (AI) is a key technology in that it enables a wide range of applications. It is already part of our everyday lives: it is the technology behind translation tools and chatbots and is used in areas like medicine. What are the challenges associated with AI? What aspects are important for the technology’s further development? Scientists from various disciplines across LMU shed some light on the following issues

--- DOC: University of Oxford | Dr Caroline Green is keeping social care human in the age of AI | University of Oxford
Dr Caroline Green is keeping social care human in the age of AI AI ethics not only concerns philosophers, computer scientists, policy makers or lawyers - AI is increasingly part of people's lives. We must ensure that it's developed, rolled out and used responsibly; in a way that does not reinforce inequalities or undermine the values that critical, life-improving actions - such as providing social care - are built on. This is a concern that drives Dr Caroline Green, Director of Research and Head of Public Engagement at the University of Oxford’s Institute for Ethics in AI, and lead of the Accelerator Fellowship Programme. Dr Green is rapidly becoming one of UK’s leading voices on the responsible use of artificial

--- DOC: Kingâ€™s College London | Michael Cook | King's College London
Dr Michael Cook Senior Lecturer in Computer Science Research interests - Computer science Contact details Biography Dr Mike Cook is a Senior Lecturer in Computer Science at King’s College London. His research focuses on artificial intelligence, creativity and applications of AI to game design and development. He is the designer of several game-designing AI systems including ANGELINA, Puck, Bluecap and Pixie, and is the author of Next Level, an upcoming book about creative technology and games. Research interests - Computational Creativity - Automated Game Design - Design & Analysis of Generative Software - General Game Playing - Computational Subjectivity Further information Archaeological Gameworld Affordances: A Grounded Theory of How Players Interpret Environmental Storytelling Smith Nicholls, F. & Cook, M., 16

--- DOC: University of Cambridge | Melvyn Weeks: AI and Machine Learning | Faculty of Economics
On a sunny morning in the Stone Room on the top floor of the Faculty, Melvyn and I took the opportunity to have a cup of tea, and take a step back from AI developments and look at the state of machine learning. After all, AI seemed to go mainstream in 2024, dominating news headlines, linkedIn feeds, and business conversations. And yet, despite witnessing significant progress over the past year, it looks as if we are just at the beginning of AI’s potential. I initially ask him why machine learning models is so important right now. He explains that AI can provide insights that that would have been impossible just a few years ago. “Machine learning models can be used


==== Cluster 13 (n=5) ====

--- DOC: University of Oxford | Expert comment: Oxford AI experts comment on the outcomes of the UK AI Safety Summit | University of Oxford
Expert comment: Oxford AI experts comment on the outcomes of the UK AI Safety Summit Over 2 days, the UK AI Safety Summit brought together approximately 150 representatives from across the globe including government leaders and ministers, and industry, academia and civil society leaders. Oxford academics comment on the outcomes. Ciaran Martin is Professor of Practice in the Management of Public Organisations, University of Oxford. “It’s easy to criticise, but don’t let the perfect be the enemy of the good. This was a good initiative and the British Government deserves credit for its global leadership. The alternative was not a better event – the alternative was nothing at all, and a repeat of the mistakes of a generation ago when

--- DOC: University of Cambridge | How Can Generative AI Impact Business Intelligence? | Cambridge Advance Online
How Can Generative AI Impact Business Intelligence? Meet Dr Russell Hunter(Opens in a new window) – Senior Teaching Associate (Online Education and Web Technology) at the Department of Engineering, University of Cambridge and the Academic Lead for the cutting-edge Cambridge Advance Online course on Leveraging Big Data for Business Intelligence.(Opens in a new window) In today’s data-driven world, generative AI is revolutionising business intelligence in ways that have never been seen before. And this technology isn’t all about getting better at crunching numbers and data – it empowers businesses to make smarter, more creative decisions at the flick of a switch. Generative AI is changing how businesses operate – going beyond just predicting outcomes to actually creating new data and

--- DOC: University of Oxford | How AI is transforming venture building and venture capital | Saïd Business School
This article below captures the highlights from our ninth Oxford Entrepreneurship Policy Roundtable. By Thomas Hellmann, Kutlu Kazanci and Mari Sako The Oxford Entrepreneurship Policy Roundtable (OXEPR) convened over 40 founders, angel investors, venture capitalists (VCs), senior executives, and academics to discuss the opportunities and challenges of building and investing in artificial intelligence (AI) ventures in Europe. The roundtable set out to understand the future of venture building and venture investing in light of the recent advances in AI. It posed and discussed the following five key questions: 1) What are the key opportunities, strategies, and business models for AI startups? Roundtable participants emphasised two factors all AI startups require to be commercially viable. First, they must generate outstanding user

--- DOC: University of Cambridge | Human brain vs AI: what makes better decisions? - News & insight - Cambridge Judge Business School
The world is rapidly tapping artificial Intelligence (AI) to transform decision-making in business, government, finance, healthcare and beyond. The technology’s analytical power is unparalleled, but a more difficult question is this: does AI actually make better decisions than humans? It’s not an easy query to answer. Research by faculty and other academics at Cambridge Judge Business School outlined in this article sheds new light on where AI outperforms human decision-making, where it fails and what business leaders must do to integrate AI most effectively. While AI excels in data-driven optimisation, risk assessment and operational efficiency, it struggles when dealing with ethics, strategic foresight and unpredictability. And then there is another human element to consider: while some people may give too

--- DOC: University of Cambridge | How AI is changing the way we work and how we’re governed - News & insight - Cambridge Judge Business School
Progress around the world on these issues has been uneven as organisations and governments experiment with this era-defining technology. But AI’s impact isn’t just technical, it’s also emotional, economic, and deeply human. In this article, we explore the topic from those different angles, reflecting on recent studies by Cambridge Judge Business School academics. Will AI replace my job? One common conversation about AI’s impact on work is often automation and job losses. But that view implies we have no control over the future, and can obscure the bigger picture. The important question now is about how we as humans choose to deploy the technology, and importantly, how humans and AI will work together. Seen through this lens, we can ask


